{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, glob\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch for Neural Netowrks\n",
    "Pytorch is one of the most popular libraries for deep learning and neural network research. We will implement two different approaches in Pytorch to explain how it works and can be used. \n",
    "\n",
    "# Linear regression as a hello world to PyTorch\n",
    "First to get familiar with the PyTorch workflow we're just going to implement a simple linear regression. The goal is to train a neural network to recover the linear regression.    \n",
    "Our equation will be of the form\n",
    "$$ y = w_0*x_0 + w_1*x_1 + w_2*x_2 $$\n",
    "\n",
    "So we will start by setting up our data. Then we'll build the simplest neural network model so represent this operation. Finally we will train it on our data to show it can recover the parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w = [0.12345, 3.4567, 6.789] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(300,3)\n",
    "y =( w[0]* x[:,0] + w[1]* x[:,1] + w[2] *x[:,2]).reshape(-1, 1)\n",
    "x_data = Variable(T.from_numpy(x.astype(np.float32)))\n",
    "y_data = Variable(T.from_numpy(y.astype(np.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we generate 300 points of data to train on, which is probably a fair amount of overkill. Now we can setup a simple linear model. We need to define the layers, and the forward step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNet(torch.nn.Module): \n",
    "\n",
    "    def __init__(self): \n",
    "        super(LinearRegressionNet, self).__init__() \n",
    "        self.linear = torch.nn.Linear(3, 1) \n",
    "\n",
    "    def forward(self, x): \n",
    "        y_pred = self.linear(x) \n",
    "        return y_pred \n",
    "\n",
    "model = LinearRegressionModel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need some way of telling our model how good it is. For this we will use the mean squared error. We will also use stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "criterion = torch.nn.MSELoss(size_average = False) \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup our training pass. For this we will present our data to the model. We then calculate a loss parameter. Next we zero out the gradient as PyTorch accumulates the gradient otherwise. Then we tell the loss to calculate the backward step, which corresponds to taking the gradient. Finally we ask our optimizer to update all of our weights and parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_history = np.zeros((10,3))\n",
    "for epoch in range(10): \n",
    "\n",
    "    # Forward pass: Compute predicted y by passing \n",
    "    # x to the model \n",
    "    parameter_history[epoch] = np.array(model.linear.weight.tolist())\n",
    "    pred_y = model(x_data) \n",
    "\n",
    "    # Compute and print loss \n",
    "    loss = criterion(pred_y, y_data) \n",
    "\n",
    "    # Zero gradients, perform a backward pass, \n",
    "    # and update the weights. \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    print('epoch {}, loss {}'.format(epoch, loss.data)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(parameter_history[:,0])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(parameter_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST model in PyTorch\n",
    "MNIST is a fairly standard dataset and provides a nice small self contained example we can use to see how a CNN would be imnplemented in PyTorch. \n",
    "\n",
    "This example was adapted from one in the PyTorch Documentation, which is a wonderful resource you should check out. \n",
    "\n",
    "First we can start by defining our network. This will consist of two convolutional layers with max pooling, followed by 2 fully connected layers. We will use the RELU function as our activation function, and use two drop out nodes to help avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first we start in the init by seeing up each of our individual layers. Then we overload the forward function to define the actual model and tie together the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two helper functions will be responsible for our training, and testing of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "args = {\n",
    "    'batch_size': 64,\n",
    "    'test_batch_size': 1000,\n",
    "    'epochs': 14,\n",
    "    'log_interval': 10,\n",
    "    'lr': 1,\n",
    "    'gamma': 0.7\n",
    "}\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=args['lr'])\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args['gamma'])\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
